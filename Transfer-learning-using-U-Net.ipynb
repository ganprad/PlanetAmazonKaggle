{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, KFold\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, Lambda, Activation,UpSampling2D\n",
    "from keras.layers.merge import add,concatenate\n",
    "from keras.optimizers import Nadam, Adam, TFOptimizer, SGD, RMSprop, Nadam\n",
    "from keras.layers import Convolution2D, Conv2D, MaxPooling2D, BatchNormalization,AveragePooling2D,GlobalAveragePooling2D\n",
    "from keras.callbacks import Callback, EarlyStopping, LearningRateScheduler, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras import backend as K\n",
    "\n",
    "from yellowfin import YFOptimizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Utils:\n",
    "\n",
    "# import spectral as sp\n",
    "# from skimage import io\n",
    "from itertools import chain\n",
    "# import spectral as sp\n",
    "# from multiprocessing import Pool\n",
    "# from skimage.transform import resize\n",
    "\n",
    "# from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import bcolz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '/media/gpk/d1c14824-369d-4058-9862-6319d625d0b8/PlanetAmazon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>haze primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>agriculture clear primary water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>clear primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>clear primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>agriculture clear habitation primary road</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_name                                       tags\n",
       "0    train_0                               haze primary\n",
       "1    train_1            agriculture clear primary water\n",
       "2    train_2                              clear primary\n",
       "3    train_3                              clear primary\n",
       "4    train_4  agriculture clear habitation primary road"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reference: https://www.kaggle.com/ekami66/0-92837-on-private-lb-solution-with-keras\n",
    "train_tiff_dir, test_tiff_dir, train_csv_file, test_mapping_file = data_helper.get_tiff_data_files_paths(path)\n",
    "labels_df = pd.read_csv(train_csv_file)\n",
    "labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMG_RESIZE=(128,128)\n",
    "SPLIT_SIZE = 0.2\n",
    "BATCH_SIZE = 128\n",
    "NB_CLASSES = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_df = pd.read_csv(train_csv_file)\n",
    "labels = sorted(set(chain.from_iterable([tags.split(\" \") for tags in labels_df['tags'].values])))\n",
    "labels_map = {l: i for i, l in enumerate(labels)}\n",
    "y_map = {v: k for k, v in labels_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_array(fname, arr):\n",
    "    c=bcolz.carray(arr, rootdir=fname, mode='w')\n",
    "    c.flush()\n",
    "\n",
    "def load_array(fname):\n",
    "    return bcolz.open(fname)[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = load_array('input/x_train_90deg_128_128_3_wjpgs.bz/')\n",
    "# x_train = load_array('input/x_train_128_128_7.bz')\n",
    "\n",
    "y_train = load_array('input/y_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_test = load_array('input/x_test_64_64_5_wjpgs_ndwi_evi.bz')\n",
    "x_test = load_array('input/x_test_90deg_128_128_3_jpgs.bz')\n",
    "gc.collect()\n",
    "# del x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train.shape,y_train.shape,x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, Y_train, Y_valid = train_test_split(x_train, y_train,test_size=SPLIT_SIZE)\n",
    "\n",
    "\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_valid = X_valid.astype(np.float32)\n",
    "\n",
    "Y_train = Y_train.astype(np.float32)\n",
    "Y_valid = Y_valid.astype(np.float32)\n",
    "\n",
    "\n",
    "# del x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_parallel(model, gpu_count):\n",
    "    #https://medium.com/@kuza55/transparent-multi-gpu-training-on-tensorflow-with-keras-8b0016fd9012\n",
    "    def get_slice(data, idx, parts):\n",
    "        shape = tf.shape(data)\n",
    "        size = tf.concat([ shape[:1] // parts, shape[1:] ],axis=0)\n",
    "        stride = tf.concat([ shape[:1] // parts, shape[1:]*0 ],axis=0)\n",
    "        start = stride * idx\n",
    "        return tf.slice(data, start, size)\n",
    "\n",
    "    outputs_all = []\n",
    "    for i in range(len(model.outputs)):\n",
    "        outputs_all.append([])\n",
    "\n",
    "    #Place a copy of the model on each GPU, each getting a slice of the batch\n",
    "    for i in range(gpu_count):\n",
    "        with tf.device('/gpu:%d' % i):\n",
    "            with tf.name_scope('tower_%d' % i) as scope:\n",
    "\n",
    "                inputs = []\n",
    "                #Slice each input into a piece for processing on this GPU\n",
    "                for x in model.inputs:\n",
    "                    input_shape = tuple(x.get_shape().as_list())[1:]\n",
    "                    slice_n = Lambda(get_slice, output_shape=input_shape, arguments={'idx':i,'parts':gpu_count})(x)\n",
    "                    inputs.append(slice_n)                \n",
    "\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                if not isinstance(outputs, list):\n",
    "                    outputs = [outputs]\n",
    "                \n",
    "                #Save all the outputs for merging back together later\n",
    "                for l in range(len(outputs)):\n",
    "                    outputs_all[l].append(outputs[l])\n",
    "\n",
    "    # merge outputs on CPU\n",
    "    with tf.device('/cpu:0'):\n",
    "        merged = []\n",
    "        for outputs in outputs_all:\n",
    "            merged.append(concatenate(outputs,axis=0))\n",
    "#             merged.append(merge(outputs, mode='concat', concat_axis=0))\n",
    "                \n",
    "            \n",
    "        return Model(input=model.inputs, output=merged)\n",
    "\n",
    "# model = make_parallel(model,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "smooth = 1e-12\n",
    "def jaccard_coef(y_true, y_pred):\n",
    "    # __author__ = Vladimir Iglovikov\n",
    "    intersection = K.sum(y_true * y_pred, axis=[0, -1, -2])\n",
    "    sum_ = K.sum(y_true + y_pred, axis=[0, -1, -2])\n",
    "\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "\n",
    "    return K.mean(jac)\n",
    "\n",
    "\n",
    "def jaccard_coef_int(y_true, y_pred):\n",
    "    # __author__ = Vladimir Iglovikov\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "\n",
    "    intersection = K.sum(y_true * y_pred_pos, axis=[0, -1, -2])\n",
    "    sum_ = K.sum(y_true + y_pred, axis=[0, -1, -2])\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "    return K.mean(jac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vgg style:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _conv_block(x,nf,size,p=0,act=True):\n",
    "    if act == True:\n",
    "        x = Conv2D(nf,(size,size),padding='same',activation='relu')(x)\n",
    "        x = Conv2D(nf,(size,size),activation='relu')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        \n",
    "#         x = MaxPooling2D(pool_size=2)(x)\n",
    "#         x = Dropout(p)(x)\n",
    "        return Activation('relu')(x) if act else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vgg_block_model(inp,nf=512,p=0.2):\n",
    "    \n",
    "    x = _conv_block(inp,nf,3,p=p)\n",
    "#     x = conv_block(inp,nf,3,(2,2))\n",
    "#     x = AveragePooling2D(pool_size=2)(x)\n",
    "    x = MaxPooling2D(pool_size=2)(x)\n",
    "    x = Dropout(p)(x)\n",
    "    \n",
    "    x = _conv_block(x,nf,3,p=p)\n",
    "#     x = AveragePooling2D(pool_size=2)(x)\n",
    "    x = MaxPooling2D(pool_size=2)(x)\n",
    "    x = Dropout(p)(x)\n",
    "    \n",
    "    x = _conv_block(x,nf,3,p=p)\n",
    "#     x = AveragePooling2D(pool_size=2)(x)\n",
    "    x = MaxPooling2D(pool_size=2)(x)\n",
    "#     x = Dropout(p)(x)\n",
    "\n",
    "    \n",
    "    x = _conv_block(x,nf,3,p=p)\n",
    "    x = MaxPooling2D(pool_size=2)(x)\n",
    "    x = Dropout(p)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _get_vgg_model(arr,p=0.2):\n",
    "    \n",
    "    nf = 32\n",
    "    inp=Input(arr.shape[1:])\n",
    "    x = _conv_block(inp,nf,3,p=p)\n",
    "#     x=conv_block(inp, nf, 3, (1,1))\n",
    "#     growth = 2\n",
    "    for i in range(3):\n",
    "        x = _conv_block(x,nf,3,p=p)\n",
    "        x = MaxPooling2D(pool_size=2)(x)\n",
    "        x = Dropout(p)(x)\n",
    "        nf = nf*2\n",
    "        \n",
    "#         nf = nf*growth\n",
    "        \n",
    "#     x= Convolution2D(17, 3, 3, activation='elu', border_mode='same')(x)\n",
    "    x = Conv2D(17,(3,3),activation='relu',padding='same')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    outp = Activation('sigmoid')(x)\n",
    "\n",
    "#     x = Flatten()(x)\n",
    "#     x = Dense(512,activation='relu')(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Dropout(0.5)(x)\n",
    "#     x = Dense(256,activation='relu')(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     x = Dropout(0.8)(x)\n",
    "#     outp = Dense(17,activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model([inp],[outp])\n",
    "    return inp,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inp,model = get_vgg_model(x_train)\n",
    "inp,vgg_model = _get_vgg_model(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg_model.compile(optimizer=Nadam(),loss='binary_crossentropy',metrics=['accuracy',jaccard_coef])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 128, 128, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_129 (Conv2D)          (None, 128, 128, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_130 (Conv2D)          (None, 126, 126, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_65 (Batc (None, 126, 126, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 126, 126, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_131 (Conv2D)          (None, 126, 126, 32)      9248      \n",
      "_________________________________________________________________\n",
      "conv2d_132 (Conv2D)          (None, 124, 124, 32)      9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_66 (Batc (None, 124, 124, 32)      128       \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 124, 124, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_38 (MaxPooling (None, 62, 62, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 62, 62, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_133 (Conv2D)          (None, 62, 62, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_134 (Conv2D)          (None, 60, 60, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_67 (Batc (None, 60, 60, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 60, 60, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_39 (MaxPooling (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_135 (Conv2D)          (None, 30, 30, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_136 (Conv2D)          (None, 28, 28, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_68 (Batc (None, 28, 28, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_40 (MaxPooling (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_137 (Conv2D)          (None, 14, 14, 17)        19601     \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_8 ( (None, 17)                0         \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 17)                0         \n",
      "=================================================================\n",
      "Total params: 326,129\n",
      "Trainable params: 325,617\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = Nadam()\n",
    "# opt = Adam()\n",
    "# opt = SGD(0.1,0.9,nesterov=True)\n",
    "# opt = TFOptimizer(YFOptimizer())\n",
    "model.compile(loss='binary_crossentropy',optimizer=opt,metrics=['binary_accuracy',jaccard_coef,jaccard_coef_int])\n",
    "K.set_value(model.optimizer.lr,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reference: fast.ai\n",
    "\n",
    "def bn(x):\n",
    "    return BatchNormalization()(x)\n",
    "\n",
    "def act(x):\n",
    "    return Activation('relu')(x)\n",
    "\n",
    "def conv(x,nf=32,wd=1e-4,p=0.2):\n",
    "    x = Conv2D(nf, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(nf, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=2)(x)\n",
    "    x = Dropout(p)(x)\n",
    "    return x\n",
    "\n",
    "def get_cov_model(arr,nblocks=4,wd=1e-4,p=0.2):\n",
    "    \n",
    "    inp = Input(shape=arr[1:])\n",
    "    x = act(bn(x))\n",
    "    \n",
    "    for i in range(nblocks):\n",
    "        x = conv(x,nf=32*i,wd=wd,p=p)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(512,activation='relu',kernel_regularizer=l2(wd))\n",
    "    x = bn(x)\n",
    "    x = Dense(17,activation='relu',kernel_regularizer=l2(wd))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model = make_parallel(model,2) \n",
    "model.fit(X_train,Y_train,batch_size=256,epochs=3,validation_data=[X_valid,Y_valid],callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LossHistory(Callback):\n",
    "    #Reference: \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.train_losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sch(epoch):\n",
    "    if epoch%20 == 0:\n",
    "        lr = 1e-3\n",
    "    if epoch%40 == 0:\n",
    "        lr = 1e-4\n",
    "    return lr\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',factor=0.1,patience=2,mode='auto',verbose=2,min_lr=1e-8)\n",
    "# earlyStopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='auto')\n",
    "# snapshot = ModelCheckpoint(filepath=\"snapshots/densenet_weights._{epoch:04d}-val_loss-{val_loss:.4f}.h5\",monitor=\"val_loss\",\n",
    "#                            save_best_only=False)\n",
    "# tqdm = TQDMNotebookCallback()\n",
    "\n",
    "filepath=\"snapshots/unet_m_dummy_128_128_3.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "# lr_sch = LearningRateScheduler(sch)\n",
    "callback_list = [ reduce_lr,checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reference: https://www.kaggle.com/drn01z3/end-to-end-baseline-with-u-net-keras\n",
    "\n",
    "def bn(x):\n",
    "    return BatchNormalization()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pool2d(x):\n",
    "    return MaxPooling2D(pool_size=(2,2))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def act(x):\n",
    "    return Activation('relu')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def up2d(x):\n",
    "    return UpSampling2D(size=(2,2))(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drp(x,p):\n",
    "    return Dropout(p)(x) if p else x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_block(x,nf,size,act=False):\n",
    "    x = Conv2D(nf,(size,size),padding='same',activation='relu')(x)\n",
    "    x = Conv2D(nf,(size,size),padding='same', activation='relu')(x)\n",
    "    return Activation(x) if act else x\n",
    "\n",
    "\n",
    "def unet_conv_block(x,nf,growth_step,positive_growth=True):\n",
    "    if positive_growth is True: nf = nf+growth_step\n",
    "    else: nf = nf-growth_step\n",
    "    return conv_block(x,nf,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unet_conv_block(x,nf,growth_step,positive_growth=True):\n",
    "    if positive_growth is True: nf = nf*growth_step\n",
    "    else: nf = nf//growth_step\n",
    "    return conv_block(x,nf,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_unet(arr,nf=32,growth_step=32):\n",
    "    inp = Input(shape=arr.shape[1:])\n",
    "    nf = 32\n",
    "#     growth_rate = 2\n",
    "    growth_step = 1\n",
    "    \n",
    "    conv1 = conv_block(bn(inp),nf,3)\n",
    "    pool1 = bn(pool2d(conv1))\n",
    "    \n",
    "    conv2 = unet_conv_block(pool1,nf*2,growth_step,positive_growth=True)\n",
    "    pool2 = bn(pool2d(conv2))\n",
    "\n",
    "    conv3 = unet_conv_block(pool2,nf*2,growth_step,positive_growth=True)\n",
    "    pool3 = bn(pool2d(conv3))\n",
    "    \n",
    "    conv4 = unet_conv_block(pool3,nf*4,growth_step,positive_growth=True)\n",
    "    pool4 = bn(pool2d(conv4))\n",
    "    \n",
    "    conv5 = unet_conv_block(pool4,nf*8,growth_step,positive_growth=True)\n",
    "    pool5 = bn(pool2d(conv5))\n",
    "    \n",
    "    #Lower most convs in the U.\n",
    "    conv6 = act(Conv2D(nf*16,(3,3),name='bottom_0',padding='same')(pool5))\n",
    "    conv6 = act(Conv2D(nf*16,(3,3),name='bottom_1',padding='same')(conv6))\n",
    "#     conv6  = conv_block(bn(pool5),nf*16,3)\n",
    "    \n",
    "    up6 = up2d(conv6)\n",
    "    up6 = bn(concatenate([up6,conv5],axis=-1))\n",
    "    \n",
    "    conv7  = unet_conv_block(up6,nf*8,growth_step,positive_growth=False)\n",
    "    up7 = up2d(conv7)\n",
    "    up7 = bn(concatenate([up7,conv4],axis=-1))\n",
    "    \n",
    "    conv8  = unet_conv_block(up7,nf*4,growth_step,positive_growth=False)\n",
    "    up8 = up2d(conv8)\n",
    "    up8 = bn(concatenate([up8,conv3],axis=-1))\n",
    "    \n",
    "    conv9  = unet_conv_block(up8,nf*2,growth_step,positive_growth=False)\n",
    "    up9 = up2d(conv9)\n",
    "    up9 = bn(concatenate([up9,conv2],axis=-1))\n",
    "    \n",
    "    #Changed this part of the Network compared to default implementation:\n",
    "    \n",
    "    conv10  = conv_block(up9,nf,3,act=False)\n",
    "    pool10 = pool2d(conv10)\n",
    "    #FCN head , added dropout\n",
    "#     fcn_op = drp(Conv2D(128,(3, 3), padding='same', activation='relu')(conv10), 0.5)\n",
    "    fcn_op = drp(Conv2D(32,(3, 3), padding='same', activation='relu')(pool10),0.1)\n",
    "    fcn_op = Conv2D(17,(3, 3), padding='same', activation='sigmoid')(fcn_op)\n",
    "    \n",
    "#     outp = GlobalAveragePooling2D()(conv10)\n",
    "    outp = GlobalAveragePooling2D()(fcn_op)\n",
    "    return inp,outp\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp,outp = get_unet(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unet_m = Model(inp,outp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_6 (InputLayer)             (None, 128, 128, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNor (None, 128, 128, 3)   12          input_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)              (None, 128, 128, 32)  896         batch_normalization_51[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)              (None, 128, 128, 32)  9248        conv2d_100[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling2D)  (None, 64, 64, 32)    0           conv2d_101[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNor (None, 64, 64, 32)    128         max_pooling2d_29[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)              (None, 64, 64, 64)    18496       batch_normalization_52[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)              (None, 64, 64, 64)    36928       conv2d_102[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling2D)  (None, 32, 32, 64)    0           conv2d_103[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNor (None, 32, 32, 64)    256         max_pooling2d_30[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)              (None, 32, 32, 64)    36928       batch_normalization_53[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)              (None, 32, 32, 64)    36928       conv2d_104[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling2D)  (None, 16, 16, 64)    0           conv2d_105[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNor (None, 16, 16, 64)    256         max_pooling2d_31[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)              (None, 16, 16, 128)   73856       batch_normalization_54[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)              (None, 16, 16, 128)   147584      conv2d_106[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling2D)  (None, 8, 8, 128)     0           conv2d_107[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNor (None, 8, 8, 128)     512         max_pooling2d_32[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)              (None, 8, 8, 256)     295168      batch_normalization_55[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)              (None, 8, 8, 256)     590080      conv2d_108[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling2D)  (None, 4, 4, 256)     0           conv2d_109[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNor (None, 4, 4, 256)     1024        max_pooling2d_33[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "bottom_0 (Conv2D)                (None, 4, 4, 512)     1180160     batch_normalization_56[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "activation_11 (Activation)       (None, 4, 4, 512)     0           bottom_0[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "bottom_1 (Conv2D)                (None, 4, 4, 512)     2359808     activation_11[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_12 (Activation)       (None, 4, 4, 512)     0           bottom_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "up_sampling2d_21 (UpSampling2D)  (None, 8, 8, 512)     0           activation_12[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)     (None, 8, 8, 768)     0           up_sampling2d_21[0][0]           \n",
      "                                                                   conv2d_109[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNor (None, 8, 8, 768)     3072        concatenate_21[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_110 (Conv2D)              (None, 8, 8, 256)     1769728     batch_normalization_57[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_111 (Conv2D)              (None, 8, 8, 256)     590080      conv2d_110[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "up_sampling2d_22 (UpSampling2D)  (None, 16, 16, 256)   0           conv2d_111[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)     (None, 16, 16, 384)   0           up_sampling2d_22[0][0]           \n",
      "                                                                   conv2d_107[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNor (None, 16, 16, 384)   1536        concatenate_22[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_112 (Conv2D)              (None, 16, 16, 128)   442496      batch_normalization_58[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_113 (Conv2D)              (None, 16, 16, 128)   147584      conv2d_112[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "up_sampling2d_23 (UpSampling2D)  (None, 32, 32, 128)   0           conv2d_113[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)     (None, 32, 32, 192)   0           up_sampling2d_23[0][0]           \n",
      "                                                                   conv2d_105[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNor (None, 32, 32, 192)   768         concatenate_23[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_114 (Conv2D)              (None, 32, 32, 64)    110656      batch_normalization_59[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_115 (Conv2D)              (None, 32, 32, 64)    36928       conv2d_114[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "up_sampling2d_24 (UpSampling2D)  (None, 64, 64, 64)    0           conv2d_115[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)     (None, 64, 64, 128)   0           up_sampling2d_24[0][0]           \n",
      "                                                                   conv2d_103[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNor (None, 64, 64, 128)   512         concatenate_24[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_116 (Conv2D)              (None, 64, 64, 32)    36896       batch_normalization_60[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_117 (Conv2D)              (None, 64, 64, 32)    9248        conv2d_116[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling2D)  (None, 32, 32, 32)    0           conv2d_117[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_118 (Conv2D)              (None, 32, 32, 32)    9248        max_pooling2d_34[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 32, 32, 32)    0           conv2d_118[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_119 (Conv2D)              (None, 32, 32, 17)    4913        dropout_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_6 (Glob (None, 17)            0           conv2d_119[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 7,951,933\n",
      "Trainable params: 7,947,895\n",
      "Non-trainable params: 4,038\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "unet_m.summary()\n",
    "plot_model(unet_m,'modified_unet.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = Nadam()\n",
    "# opt = TFOptimizer(YFOptimizer())\n",
    "unet_m.compile(loss='binary_crossentropy',optimizer=opt,metrics=['accuracy',jaccard_coef])\n",
    "# K.set_value(unet_m.optimizer.learn_rate,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_m.fit(X_train,Y_train,validation_data=[X_valid,Y_valid],batch_size=128,epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unet_m_predictions = unet_m.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Cross-Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model,X_valid,Y_valid,thres=0.18):\n",
    "        p_valid = model.predict(X_valid)\n",
    "        \n",
    "        thresholded_preds = np.array(p_valid) > thres\n",
    "        \n",
    "        return fbeta_score(Y_valid, thresholded_preds, beta=2, average='samples')#,thresholded_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_cv(model,X, y, nfolds=2,lr=1e-3, epochs=1, batch_size=128, split_size=0.1,train_callbacks=()):\n",
    "        '''Input compiled model and data(X,y)'''\n",
    "        \n",
    "        sum_scores = 0\n",
    "        num_fold = 0\n",
    "        y_val_preds_dict = {}\n",
    "        \n",
    "        kf = KFold(n_splits=nfolds)\n",
    "\n",
    "        for train_index, test_index in kf.split(X,y):\n",
    "            gc.collect()\n",
    "            X_train = X[train_index]\n",
    "            Y_train = y[train_index]\n",
    "            X_valid = X[test_index]\n",
    "            Y_valid = y[test_index]\n",
    "            \n",
    "            gc.collect()\n",
    "            \n",
    "            num_fold += 1\n",
    "            print('==Start Fold number {} from {}'.format(num_fold, nfolds))\n",
    "            print('===Split train: ', len(X_train), len(Y_train))\n",
    "            print('===Split valid: ', len(X_valid), len(Y_valid))\n",
    "\n",
    "            history = LossHistory()  \n",
    "            earlyStopping = EarlyStopping(monitor='val_loss', patience=3, verbose=0, mode='auto')\n",
    "            callbacks = [history,earlyStopping]\n",
    "            \n",
    "            K.set_value(model.optimizer.lr,lr)\n",
    "            model.fit(X_train, Y_train,\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=epochs,\n",
    "                            verbose=1,\n",
    "                            validation_data=(X_valid, Y_valid),\n",
    "                            callbacks=[history,earlyStopping,*callbacks])\n",
    "            gc.collect()\n",
    "            fbeta = evaluate_model(model,X_valid,Y_valid,thres=0.2)\n",
    "\n",
    "            \n",
    "            print('fbeta score: ', fbeta)                     \n",
    "            sum_scores += fbeta*len(test_index)\n",
    "            \n",
    "        score = sum_scores /  len(x_train)\n",
    "        print(\"fbeta val score on whole train set: \", score)\n",
    "\n",
    "        return [history.train_losses,history.val_losses, score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unet_m.load_weights('snapshots/CV_10_fold_unet_m_128_128_3_jpgs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# unet.fit(X_train, nb_epoch=100)\n",
    "unet_m.save_weights('snapshots/CV_10_fold_unet_m_128_128_3_jpgs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Debugging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "inp = Input(x_train.shape[1:])\n",
    "nf = 32\n",
    "growth_step = 32\n",
    "    \n",
    "x = bn(inp)\n",
    "x = act(Conv2D(nf,(3,3),padding='same')(x))\n",
    "x = act(Conv2D(nf,(3,3),padding='same')(x))\n",
    "x = pool2d(x)\n",
    "pool1 = bn(x)\n",
    "conv2 = unet_conv_block(pool1,nf,growth_step,positive_growth=True)\n",
    "# pool2 = pool2d(conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = act(Conv2D(32,(3,3),padding='same')(bn(Input(x_train.shape[1:]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather_labels = ['clear', 'partly_cloudy', 'haze', 'cloudy']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "climate_idx = np.array([labels_map[label] for label in weather_labels])\n",
    "climate_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train[:,idx].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drop_columns(y,drop_idx):\n",
    "    return y[:,idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_cli_train = drop_columns(Y_train,climate_idx)\n",
    "Y_cli_valid = drop_columns(Y_valid,climate_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==Start Fold number 1 from 10\n",
      "===Split train:  36431 36431\n",
      "===Split valid:  4048 4048\n",
      "Train on 36431 samples, validate on 4048 samples\n",
      "Epoch 1/10\n",
      "36431/36431 [==============================] - 110s - loss: 0.6451 - acc: 0.7423 - jaccard_coef: 0.2162 - val_loss: 0.6919 - val_acc: 0.8182 - val_jaccard_coef: 0.1455\n",
      "Epoch 2/10\n",
      "36431/36431 [==============================] - 110s - loss: 0.6351 - acc: 0.8617 - jaccard_coef: 0.2272 - val_loss: 0.6463 - val_acc: 0.8777 - val_jaccard_coef: 0.2056\n",
      "Epoch 3/10\n",
      "36431/36431 [==============================] - 110s - loss: 0.6325 - acc: 0.8910 - jaccard_coef: 0.2300 - val_loss: 0.6353 - val_acc: 0.9187 - val_jaccard_coef: 0.2246\n",
      "Epoch 4/10\n",
      "36431/36431 [==============================] - 111s - loss: 0.6307 - acc: 0.9000 - jaccard_coef: 0.2320 - val_loss: 0.6553 - val_acc: 0.9060 - val_jaccard_coef: 0.2170\n",
      "Epoch 5/10\n",
      "36431/36431 [==============================] - 110s - loss: 0.6294 - acc: 0.9088 - jaccard_coef: 0.2333 - val_loss: 0.6372 - val_acc: 0.9018 - val_jaccard_coef: 0.2179\n",
      "fbeta score:  0.482340327071\n",
      "==Start Fold number 2 from 10\n",
      "===Split train:  36431 36431\n",
      "===Split valid:  4048 4048\n",
      "Train on 36431 samples, validate on 4048 samples\n",
      "Epoch 1/10\n",
      "36431/36431 [==============================] - 110s - loss: 0.6296 - acc: 0.9141 - jaccard_coef: 0.2331 - val_loss: 0.6498 - val_acc: 0.9082 - val_jaccard_coef: 0.2016\n",
      "Epoch 2/10\n",
      "36431/36431 [==============================] - 110s - loss: 0.6272 - acc: 0.9173 - jaccard_coef: 0.2363 - val_loss: 0.6626 - val_acc: 0.8926 - val_jaccard_coef: 0.2157\n",
      "Epoch 3/10\n",
      "36431/36431 [==============================] - 110s - loss: 0.6259 - acc: 0.9223 - jaccard_coef: 0.2380 - val_loss: 0.6762 - val_acc: 0.8051 - val_jaccard_coef: 0.1732\n",
      "fbeta score:  0.483649122628\n",
      "==Start Fold number 3 from 10\n",
      "===Split train:  36431 36431\n",
      "===Split valid:  4048 4048\n",
      "Train on 36431 samples, validate on 4048 samples\n",
      "Epoch 1/10\n",
      "36431/36431 [==============================] - 110s - loss: 0.6253 - acc: 0.9246 - jaccard_coef: 0.2387 - val_loss: 0.6463 - val_acc: 0.8870 - val_jaccard_coef: 0.2369\n",
      "Epoch 2/10\n",
      "36431/36431 [==============================] - 111s - loss: 0.6245 - acc: 0.9275 - jaccard_coef: 0.2394 - val_loss: 0.6527 - val_acc: 0.9035 - val_jaccard_coef: 0.2097\n",
      "Epoch 3/10\n",
      "36431/36431 [==============================] - 110s - loss: 0.6240 - acc: 0.9276 - jaccard_coef: 0.2399 - val_loss: 0.6360 - val_acc: 0.9237 - val_jaccard_coef: 0.2425\n",
      "Epoch 4/10\n",
      "36431/36431 [==============================] - 110s - loss: 0.6234 - acc: 0.9316 - jaccard_coef: 0.2405 - val_loss: 0.6553 - val_acc: 0.9068 - val_jaccard_coef: 0.2278\n",
      "Epoch 5/10\n",
      "36431/36431 [==============================] - 110s - loss: 0.6276 - acc: 0.9265 - jaccard_coef: 0.2360 - val_loss: 0.6553 - val_acc: 0.9129 - val_jaccard_coef: 0.2191\n",
      "fbeta score:  0.484460046289\n",
      "==Start Fold number 4 from 10\n",
      "===Split train:  36431 36431\n",
      "===Split valid:  4048 4048\n",
      "Train on 36431 samples, validate on 4048 samples\n",
      "Epoch 1/10\n",
      "36431/36431 [==============================] - 111s - loss: 0.6237 - acc: 0.9329 - jaccard_coef: 0.2404 - val_loss: 0.6265 - val_acc: 0.9266 - val_jaccard_coef: 0.2341\n",
      "Epoch 2/10\n",
      "36431/36431 [==============================] - 111s - loss: 0.6228 - acc: 0.9335 - jaccard_coef: 0.2414 - val_loss: 0.6611 - val_acc: 0.9060 - val_jaccard_coef: 0.2350\n",
      "Epoch 3/10\n",
      "36431/36431 [==============================] - 111s - loss: 0.6224 - acc: 0.9351 - jaccard_coef: 0.2418 - val_loss: 0.6576 - val_acc: 0.8892 - val_jaccard_coef: 0.1962\n",
      "fbeta score:  0.483692842179\n",
      "==Start Fold number 5 from 10\n",
      "===Split train:  36431 36431\n",
      "===Split valid:  4048 4048\n",
      "Train on 36431 samples, validate on 4048 samples\n",
      "Epoch 1/10\n",
      "36431/36431 [==============================] - 110s - loss: 0.6221 - acc: 0.9357 - jaccard_coef: 0.2421 - val_loss: 0.8416 - val_acc: 0.8548 - val_jaccard_coef: 0.1720\n",
      "Epoch 2/10\n",
      "36431/36431 [==============================] - 110s - loss: 0.6218 - acc: 0.9359 - jaccard_coef: 0.2422 - val_loss: 0.6255 - val_acc: 0.9341 - val_jaccard_coef: 0.2342\n",
      "Epoch 3/10\n",
      "36431/36431 [==============================] - 111s - loss: 0.6214 - acc: 0.9368 - jaccard_coef: 0.2427 - val_loss: 0.6428 - val_acc: 0.9211 - val_jaccard_coef: 0.2264\n",
      "Epoch 4/10\n",
      "36431/36431 [==============================] - 110s - loss: 0.6211 - acc: 0.9373 - jaccard_coef: 0.2430 - val_loss: 0.6316 - val_acc: 0.9266 - val_jaccard_coef: 0.2414\n",
      "fbeta score:  0.484159699068\n",
      "==Start Fold number 6 from 10\n",
      "===Split train:  36431 36431\n",
      "===Split valid:  4048 4048\n",
      "Train on 36431 samples, validate on 4048 samples\n",
      "Epoch 1/10\n",
      "36431/36431 [==============================] - 111s - loss: 0.6208 - acc: 0.9388 - jaccard_coef: 0.2432 - val_loss: 0.6380 - val_acc: 0.9229 - val_jaccard_coef: 0.2295\n",
      "Epoch 2/10\n",
      "36431/36431 [==============================] - 111s - loss: 0.6206 - acc: 0.9390 - jaccard_coef: 0.2435 - val_loss: 0.6241 - val_acc: 0.9345 - val_jaccard_coef: 0.2458\n",
      "Epoch 3/10\n",
      "36431/36431 [==============================] - 111s - loss: 0.6202 - acc: 0.9391 - jaccard_coef: 0.2437 - val_loss: 0.6312 - val_acc: 0.9243 - val_jaccard_coef: 0.2312\n",
      "Epoch 4/10\n",
      "36431/36431 [==============================] - 110s - loss: 0.6199 - acc: 0.9396 - jaccard_coef: 0.2441 - val_loss: 0.6238 - val_acc: 0.9292 - val_jaccard_coef: 0.2406\n",
      "Epoch 5/10\n",
      "36431/36431 [==============================] - 110s - loss: 0.6195 - acc: 0.9406 - jaccard_coef: 0.2443 - val_loss: 0.6341 - val_acc: 0.9297 - val_jaccard_coef: 0.2297\n",
      "Epoch 6/10\n",
      "36431/36431 [==============================] - 111s - loss: 0.6194 - acc: 0.9403 - jaccard_coef: 0.2445 - val_loss: 0.6234 - val_acc: 0.9360 - val_jaccard_coef: 0.2444\n",
      "Epoch 7/10\n",
      "36431/36431 [==============================] - 111s - loss: 0.6191 - acc: 0.9411 - jaccard_coef: 0.2447 - val_loss: 0.6239 - val_acc: 0.9366 - val_jaccard_coef: 0.2398\n",
      "Epoch 8/10\n",
      "36431/36431 [==============================] - 111s - loss: 0.6187 - acc: 0.9413 - jaccard_coef: 0.2450 - val_loss: 0.6313 - val_acc: 0.9338 - val_jaccard_coef: 0.2319\n",
      "fbeta score:  0.484424856197\n",
      "==Start Fold number 7 from 10\n",
      "===Split train:  36431 36431\n",
      "===Split valid:  4048 4048\n",
      "Train on 36431 samples, validate on 4048 samples\n",
      "Epoch 1/10\n",
      "36431/36431 [==============================] - 111s - loss: 0.6188 - acc: 0.9416 - jaccard_coef: 0.2450 - val_loss: 0.6299 - val_acc: 0.9282 - val_jaccard_coef: 0.2450\n",
      "Epoch 2/10\n",
      "36431/36431 [==============================] - 111s - loss: 0.6184 - acc: 0.9416 - jaccard_coef: 0.2453 - val_loss: 0.6224 - val_acc: 0.9380 - val_jaccard_coef: 0.2442\n",
      "Epoch 3/10\n",
      "36431/36431 [==============================] - 111s - loss: 0.6182 - acc: 0.9426 - jaccard_coef: 0.2455 - val_loss: 0.6219 - val_acc: 0.9379 - val_jaccard_coef: 0.2386\n",
      "Epoch 4/10\n",
      "36431/36431 [==============================] - 111s - loss: 0.6176 - acc: 0.9424 - jaccard_coef: 0.2464 - val_loss: 0.7452 - val_acc: 0.8671 - val_jaccard_coef: 0.1882\n",
      "Epoch 5/10\n",
      "36431/36431 [==============================] - 111s - loss: 0.6171 - acc: 0.9428 - jaccard_coef: 0.2469 - val_loss: 0.6220 - val_acc: 0.9408 - val_jaccard_coef: 0.2440\n",
      "fbeta score:  0.484771808647\n",
      "==Start Fold number 8 from 10\n",
      "===Split train:  36431 36431\n",
      "===Split valid:  4048 4048\n",
      "Train on 36431 samples, validate on 4048 samples\n",
      "Epoch 1/10\n",
      "36431/36431 [==============================] - 111s - loss: 0.6171 - acc: 0.9438 - jaccard_coef: 0.2472 - val_loss: 0.6863 - val_acc: 0.8941 - val_jaccard_coef: 0.2008\n",
      "Epoch 2/10\n",
      "36431/36431 [==============================] - 111s - loss: 0.6166 - acc: 0.9436 - jaccard_coef: 0.2476 - val_loss: 0.8890 - val_acc: 0.9038 - val_jaccard_coef: 0.2316\n",
      "Epoch 3/10\n",
      "36431/36431 [==============================] - 111s - loss: 0.6162 - acc: 0.9436 - jaccard_coef: 0.2480 - val_loss: 0.6248 - val_acc: 0.9411 - val_jaccard_coef: 0.2375\n",
      "Epoch 4/10\n",
      "36431/36431 [==============================] - 111s - loss: 0.6159 - acc: 0.9444 - jaccard_coef: 0.2482 - val_loss: 0.6373 - val_acc: 0.9343 - val_jaccard_coef: 0.2275\n",
      "Epoch 5/10\n",
      "36431/36431 [==============================] - 111s - loss: 0.6160 - acc: 0.9445 - jaccard_coef: 0.2480 - val_loss: 0.6317 - val_acc: 0.9372 - val_jaccard_coef: 0.2465\n",
      "fbeta score:  0.480766272226\n",
      "==Start Fold number 9 from 10\n",
      "===Split train:  36431 36431\n",
      "===Split valid:  4048 4048\n",
      "Train on 36431 samples, validate on 4048 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36431/36431 [==============================] - 111s - loss: 0.6159 - acc: 0.9448 - jaccard_coef: 0.2481 - val_loss: 0.6351 - val_acc: 0.9162 - val_jaccard_coef: 0.2273\n",
      "Epoch 2/10\n",
      "36431/36431 [==============================] - 112s - loss: 0.6158 - acc: 0.9454 - jaccard_coef: 0.2482 - val_loss: 0.6290 - val_acc: 0.9303 - val_jaccard_coef: 0.2367\n",
      "Epoch 3/10\n",
      "36431/36431 [==============================] - 111s - loss: 0.6153 - acc: 0.9451 - jaccard_coef: 0.2486 - val_loss: 0.6201 - val_acc: 0.9426 - val_jaccard_coef: 0.2414\n",
      "Epoch 4/10\n",
      "36431/36431 [==============================] - 111s - loss: 0.6149 - acc: 0.9457 - jaccard_coef: 0.2488 - val_loss: 0.6467 - val_acc: 0.9242 - val_jaccard_coef: 0.2303\n",
      "Epoch 5/10\n",
      "36431/36431 [==============================] - 111s - loss: 0.6149 - acc: 0.9461 - jaccard_coef: 0.2488 - val_loss: 0.6258 - val_acc: 0.9352 - val_jaccard_coef: 0.2427\n",
      "fbeta score:  0.485202774431\n",
      "==Start Fold number 10 from 10\n",
      "===Split train:  36432 36432\n",
      "===Split valid:  4047 4047\n",
      "Train on 36432 samples, validate on 4047 samples\n",
      "Epoch 1/10\n",
      "36432/36432 [==============================] - 111s - loss: 0.6150 - acc: 0.9462 - jaccard_coef: 0.2489 - val_loss: 0.6166 - val_acc: 0.9458 - val_jaccard_coef: 0.2435\n",
      "Epoch 2/10\n",
      "36432/36432 [==============================] - 112s - loss: 0.6144 - acc: 0.9460 - jaccard_coef: 0.2493 - val_loss: 0.6367 - val_acc: 0.9298 - val_jaccard_coef: 0.2215\n",
      "Epoch 3/10\n",
      "20480/36432 [===============>..............] - ETA: 46s - loss: 0.6139 - acc: 0.9470 - jaccard_coef: 0.2498"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-dbabc8958b66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                                                            \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearn_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                                            \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                                                            train_callbacks=callback_list)\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_train_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mval_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_val_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-107-a741d47541d9>\u001b[0m in \u001b[0;36mrun_cv\u001b[0;34m(model, X, y, nfolds, lr, epochs, batch_size, split_size, train_callbacks)\u001b[0m\n\u001b[1;32m     32\u001b[0m                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                             \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                             callbacks=[history,earlyStopping,*callbacks])\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mfbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthres\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazon/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1428\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazon/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1077\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazon/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazon/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazon/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazon/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/anaconda3/envs/amazon/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazon/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = [], []\n",
    "epochs_arr = [10,5,5]\n",
    "learn_rates = [0.001,0.0001,0.00001]#, 0.00001]\n",
    "\n",
    "\n",
    "for learn_rate, epochs in zip(learn_rates, epochs_arr):\n",
    "    tmp_train_losses, tmp_val_losses, score = run_cv(vgg_model,x_train, y_train,\n",
    "                                                           nfolds=10,\n",
    "                                                           lr=learn_rate, epochs=epochs, \n",
    "                                                           batch_size=BATCH_SIZE,\n",
    "                                                           train_callbacks=callback_list)\n",
    "    train_losses.append(tmp_train_losses)\n",
    "    val_losses.append(tmp_val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "40479/40479 [==============================] - 112s - loss: 0.1816 - acc: 0.9270 - jaccard_coef: 0.5258   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa15994b438>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet_m.fit(x_train,y_train,batch_size=128,epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Checking predictions :\n",
    "\n",
    "#Looking at borderline cases(low max and/or high low values)\n",
    "thres = 0.4\n",
    "# high_min = {i:min(pred) for i,pred in enumerate(predictions) if min(pred)>=1e-3}\n",
    "low_max = {i:max(pred) for i,pred in enumerate(predictions) if max(pred)<=0.45}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "high_min = {i:min(pred) for i,pred in enumerate(predictions) if max(pred)<=0.45}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(high_min))\n",
    "high_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Checking intersections:\n",
    "\n",
    "len(set(low_max.keys()).intersection(set(high_min.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(low_max))\n",
    "low_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "{i:pred for i,pred in enumerate(dense_unet_predictions) if max(pred)<=thres}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = np.array(list(low_max.keys())); print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Edge cases dict:\n",
    "thres = 0.2\n",
    "edge_cases_dict = {idx[i]:[y_map[j] for j,value in enumerate(pred) if value>thres] for i,pred in \n",
    "                   enumerate(predictions[idx,:])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "edge_cases_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thres = 0.2\n",
    "y_pred_list = [' '.join([y_map[j] for j,value in enumerate(pred) if value>thres]) for i,pred in \n",
    "               enumerate(predictions_90)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['clear primary',\n",
       " 'clear primary',\n",
       " 'agriculture clear primary road',\n",
       " 'clear primary water',\n",
       " 'clear primary road',\n",
       " 'haze primary',\n",
       " 'clear primary',\n",
       " 'agriculture partly_cloudy primary water',\n",
       " 'clear primary',\n",
       " 'partly_cloudy primary']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_dict = {op[0].split('.')[0]:op[1] for i,op in enumerate(zip(os.listdir(test_tiff_dir),y_pred_list))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_df = pd.DataFrame(list(pred_dict.items()),columns=['image_name', 'tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_df.to_csv('u_net_m_preds_da_1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
